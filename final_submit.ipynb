{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING - Initial steps\n",
    "\n",
    "\n",
    "The below code reads in the raw CSV kickstarter data and then\n",
    "* Removes stopwords and punctuation\n",
    "* Lemmatizes all words (eg mice -> mouse, running -> run)\n",
    "* Writes the processed data to JSON format\n",
    "\n",
    "Lemmatization is a fairly intensive task and with a dataset of over 200,000 this needed to run overnight. \n",
    "\n",
    "The recommended usage is to skip this cell and work off of the already lemmatized JSON outputs included in the repository. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3b759f222ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Read in all of the CSV files, concatenate them into one dataset. \n",
    "csv_files = glob.glob(\"kickstarter_data/Kickstarter*\")\n",
    "subsets = []\n",
    "for csv_file in csv_files:\n",
    "    subsets.append(pd.read_csv(csv_file))\n",
    "dset = pd.concat(subsets)\n",
    "\n",
    "# Take in text and return an array of lemmatized, tokenized, and stopword-removed word features\n",
    "def text_features(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    tokens = nlp(text)\n",
    "    feats = []    \n",
    "    for tok in tokens: # lemmatize words that are not pronouns \n",
    "        feats.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    feats = [feat for feat in feats if feat not in stopwords]\n",
    "    return feats\n",
    "\n",
    "# Goes through every kickstarter project in the dataset, and writes it back to disk in json format. \n",
    "dump = 0\n",
    "projects = []\n",
    "for idx, item in dset.iterrows():\n",
    "    project = {'pledged' : item['pledged'] * item['fx_rate'],\n",
    "              'goal'    : item['goal'] * item['fx_rate'],\n",
    "              'category': json.loads(item['category'])['slug'].split(\"/\"), \n",
    "              'text'    : str(item['name']) + \" \" + str(item['blurb']),\n",
    "              'text_feats'    : text_features(str(item['name']) + \" \" + str(item['blurb']))}\n",
    "    projects.append(project)\n",
    "    if idx % 1000 == 999:\n",
    "        with open('kickstarter_data/data' + str(dump) + '.json', 'w') as outfile:  \n",
    "            json.dump(projects, outfile)\n",
    "            dump += 1\n",
    "            projects = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning - more interesting stuff\n",
    "\n",
    "These are all the functions needed to convert the lemmatized/tokenized word features into a usable format for scikitlearn's regression models\n",
    "\n",
    "Intended usage is to start running from here, downloading the json formatted features that are inside the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import nltk\n",
    "import random\n",
    "# A constant for the top most useful uni-, bi-, and trigrams. Edit this to use more or less of each gram type. \n",
    "most_useful = {\"uni\": 200, \"bi\": 100, \"tri\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# **read_data()**\n",
    "# - **Func Desc:**<br>\n",
    "#     This function reads in the entire Kickstarter dataset from json files in the \"kickstarter_data\" directory.\n",
    "# - **Return:**<br>\n",
    "#     An nx5 list of projects, where n represents the total number of projects. Note that there are 5 attributes of a single project: the category, text, pledged amount, goal amount, and text_features.\n",
    "def read_data():\n",
    "    projects = []\n",
    "\n",
    "    # Read in data\n",
    "    json_files = glob.glob(\"kickstarter_data/data*\")\n",
    "\n",
    "    for json_file in json_files:\n",
    "        projects += json.load(open(json_file, 'r'))\n",
    "\n",
    "    return projects    \n",
    "\n",
    "# **grams_by_project(*list text*)**\n",
    "# - **Func Desc:**<br>\n",
    "#     This function will find all the unigrams, bigrams, and trigrams in the given *text*.\n",
    "# - **Return:**<br>\n",
    "#     A dictionary containing all unigrams, bigrams, and trigrams, \n",
    "#     where the corresponding keys are \"uni\", \"bi\" and \"tri\"\n",
    "def grams_by_project(text):\n",
    "    grams = {}\n",
    "    \n",
    "    all_words = []\n",
    "    all_bigrams = []\n",
    "    all_trigrams = []\n",
    "    \n",
    "    prev_prev = ''\n",
    "    prev_word = '<SOS>' # Start of sentence\n",
    "\n",
    "    for w in text:\n",
    "        # Ignore empty strings and apostrophe+s ending\n",
    "        if w == \"'s\" or w == 'â€™s' or w == '':  \n",
    "            continue\n",
    "\n",
    "        all_words.append(w)\n",
    "        all_bigrams.append(prev_word + \" \" + w)\n",
    "\n",
    "        if prev_prev != '':\n",
    "            all_trigrams.append(prev_prev + \" \" + prev_word + \" \" + w)\n",
    "\n",
    "        prev_prev = prev_word\n",
    "        prev_word = w\n",
    "    \n",
    "    grams[\"uni\"] = all_words\n",
    "    grams[\"bi\"]  = all_bigrams\n",
    "    grams[\"tri\"] = all_trigrams\n",
    "    \n",
    "    return grams\n",
    "\n",
    "# **grams_by_category(*string category*, **[optional]** *int n*, **[optional]** *boolean do_print*)**\n",
    "# - **Func Desc:**<br>\n",
    "#     This function will find the unigrams, bigrams, and trigrams in the given *category*. If *do_print* is set, then the *n* most common unigrams, bigrams, and trigrams will be displayed.\n",
    "# - **Return:**<br>\n",
    "#     A dictionary containing all unigrams, bigrams, and trigrams, \n",
    "#     where the corresponding keys are \"uni\", \"bi\" and \"tri\"\n",
    "def grams_by_category(projects, category, n=15, do_print=True):\n",
    "    grams = {}\n",
    "    \n",
    "    all_words = []\n",
    "    all_bigrams = []\n",
    "    all_trigrams = []\n",
    "    \n",
    "    for project in projects:\n",
    "        \n",
    "        # Change this to check out a different sub-category, \n",
    "        # 'all' will check the entire thing\n",
    "        if category != 'all' and category not in project['category']: \n",
    "            continue\n",
    "\n",
    "        prev_prev = ''\n",
    "        prev_word = '<SOS>' # Start of sentence\n",
    "        \n",
    "        proj_grams = grams_by_project(project['text_feats'])\n",
    "            \n",
    "        all_words += proj_grams[\"uni\"]\n",
    "        all_bigrams += proj_grams[\"bi\"]\n",
    "        all_trigrams += proj_grams[\"tri\"]\n",
    "        \n",
    "    grams[\"uni\"] = nltk.FreqDist(all_words)\n",
    "    grams[\"bi\"]  = nltk.FreqDist(all_bigrams)\n",
    "    grams[\"tri\"] = nltk.FreqDist(all_trigrams)\n",
    "    \n",
    "    if do_print:\n",
    "        print(\"-- UNIGRAMS --\")\n",
    "        all_words = nltk.FreqDist(all_words)\n",
    "        \n",
    "        for word in all_words.most_common(n):\n",
    "            print(word[0], \"\\t\", word[1])\n",
    "\n",
    "        print()\n",
    "        print(\"-- BIGRAMS --\")\n",
    "        all_bigrams = nltk.FreqDist(all_bigrams)\n",
    "        \n",
    "        for bigram in all_bigrams.most_common(n):\n",
    "            print(bigram[0], \"\\t\", bigram[1])\n",
    "\n",
    "        print()\n",
    "        print(\"-- TRIGRAMS --\")\n",
    "        all_trigrams = nltk.FreqDist(all_trigrams)\n",
    "        \n",
    "        for trigram in all_trigrams.most_common(n):\n",
    "            print(trigram[0], \"\\t\", trigram[1])\n",
    "    \n",
    "    return grams\n",
    "\n",
    "\n",
    "# **map_gram_to_idx(*dictionary grams*, **[optional]** num_uni, **[optional]** num_bi, **[optional]** num_tri)**\n",
    "# - **Func Desc:**<br>\n",
    "#     Given a dictionary of unigrams, bigrams, and trigrams, this function maps each gram to a unique index. We will later use this to vectorize the most unique uni-, bi-, and trigrams. Note that *num_uni* represents the \"n\" most common unigrams, and similarily for *num_bi* and *num_tri*.\n",
    "# - **Return:**<br>\n",
    "#     A dictionary containing all unigrams, bigrams, and trigrams mapped to a unique integer index.\n",
    "def map_gram_to_idx(grams_dict, num_uni=most_useful[\"uni\"], \n",
    "                      num_bi=most_useful[\"bi\"], \n",
    "                      num_tri=most_useful[\"tri\"]):\n",
    "    gram_to_idx = {}\n",
    "    count = 0\n",
    "    \n",
    "    for word, _ in grams_dict[\"uni\"].most_common(num_uni):\n",
    "        gram_to_idx[word] = count\n",
    "        count += 1\n",
    "\n",
    "    for phrase, _ in grams_dict[\"bi\"].most_common(num_bi):\n",
    "        gram_to_idx[phrase] = count\n",
    "        count += 1\n",
    "\n",
    "    for phrase, _ in grams_dict[\"tri\"].most_common(num_tri):\n",
    "        gram_to_idx[phrase] = count\n",
    "        count += 1\n",
    "        \n",
    "    return gram_to_idx\n",
    "\n",
    "\n",
    "# **vectorize(*list text*, *dictionary gram_to_idx*)**\n",
    "# - **Func Desc:**<br>\n",
    "#     For each uni-, bi-, and trigram in *text*, this function will indicate whether each gram is present in *gram_to_idx* (1: present; 0: not present). Note that *gram_to_idx* represents a mapping of the n most common uni-, bi-, and trigrams of a particular project category.\n",
    "# - **Return:**<br>\n",
    "#     A list of 0s and 1s, where 0 indicates that the gram found at *gram_to_idx[i]* is not present in *text* and 1 means that the gram is present.\n",
    "def vectorize(project, gram_to_idx):\n",
    "    text = project['text_feats']\n",
    "    feats = [0] * (len(gram_to_idx) + 1)\n",
    "    feats[-1] = project['goal']\n",
    "    proj_grams = grams_by_project(text)\n",
    "        \n",
    "    for _, grams in proj_grams.items():\n",
    "        for g in grams:\n",
    "            if g in gram_to_idx:\n",
    "                feats[gram_to_idx[g]] = 1\n",
    "               \n",
    "    return feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all of the data, and then find the top 10 most common kickstarter categories to analyze. \n",
    "We made the decision to limit analysis to category by category, so that we could get a more fine-grained description, indicating what kinds of projects are the most appealing for each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feats(projects, category):\n",
    "    \n",
    "    # Find and print most common unigrams and bigrams in category\n",
    "    grams = grams_by_category(projects, category, do_print=False)\n",
    "\n",
    "    # Map grams to unique index for easy vectorization\n",
    "    grams_to_idx = map_gram_to_idx(grams)\n",
    "\n",
    "    # Map unique index to gram to quickly convert vectorization to txt\n",
    "    idx_to_grams = [0] * len(grams_to_idx)\n",
    "\n",
    "    for gram, idx in grams_to_idx.items():\n",
    "        idx_to_grams[idx] = gram\n",
    "        \n",
    "    # Build feats + labels for model training\n",
    "    feats = []\n",
    "    labels = []\n",
    "\n",
    "    for project in projects:\n",
    "        if project['category'][0] == category or category == 'all':\n",
    "            encoding = vectorize(project, grams_to_idx)\n",
    "\n",
    "            # Label represents amt pledged\n",
    "            label = project['pledged']\n",
    "\n",
    "            feats.append(encoding)\n",
    "            labels.append(label)\n",
    "            \n",
    "    return idx_to_grams, feats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(projects, category, validate=False):\n",
    "            \n",
    "    idx_to_grams, feats, labels = build_feats(projects, category)\n",
    "            \n",
    "    # 90-10 split feats and labels; 90% training data and 10% test data\n",
    "    feats_train = feats[:int(len(feats) * .9)]\n",
    "    feats_test  = feats[int(len(feats) * .9):]\n",
    "\n",
    "    labels_train = labels[:int(len(labels) * .9)]\n",
    "    labels_test  = labels[int(len(labels) * .9):]\n",
    "    \n",
    "    model = linear_model.Ridge(alpha=1000)     # Initialize model\n",
    "    model.fit(feats_train, labels_train)       # Train model\n",
    "    \n",
    "    # If validate=True, then validate model using 10% of data\n",
    "    if validate:\n",
    "        predictions = model.predict(feats_test)\n",
    "        \n",
    "        MSE = mean_squared_error(predictions, labels_test)\n",
    "        print(\"MSE:\", MSE)\n",
    "        \n",
    "    word_corrs = sorted(zip(idx_to_grams, model.coef_), key=lambda t: -t[1])\n",
    "        \n",
    "    return model, word_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_categories = []\n",
    "\n",
    "# Get list of all possible categories\n",
    "for project in projects:\n",
    "    for category in project['category']:\n",
    "        all_categories.append(category)\n",
    "        \n",
    "all_categories = nltk.FreqDist(all_categories)\n",
    "\n",
    "# Get top-10 categories\n",
    "top_10_categories = [category[0] for category in all_categories.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = {}\n",
    "coefs = []\n",
    "\n",
    "for category in top_10_categories:\n",
    "    temp = {}\n",
    "    \n",
    "    LR, corrs = create_model(projects,category)\n",
    "    \n",
    "    temp['grams'] = [t[0] for t in corrs]\n",
    "    temp['monetary_impact'] = [t[1] for t in corrs]\n",
    "    \n",
    "    coefs.append([category, LR.intercept_,LR.coef_[-1]])\n",
    "    \n",
    "    grams[category] = pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams_df = pd.concat(grams, axis=1, keys=top_10_categories)\n",
    "coefs_df = pd.DataFrame.from_records(coefs, columns=['category', 'intercept', 'goal_v_raised'], index='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams_df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs_df.style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
