# Cogs108_Repo
 
Preprocessing: \n
Original dataset from https://webrobots.io/kickstarter-datasets/ \n
Used spacy and NLTK to tokenize, lemmatize, and remove stopwords. \n 
  Tokeization -> Break up sentences into individual words \n
  Lemmatization -> Reducing words to their base words, e.g. mice -> mouse, printed -> print \n
  Stopword removal -> Removing words and punctuation that aren't meaningful on their own (eg, removing words like 'and, to, the') \n
