# Cogs108_Repo
 
Preprocessing: 

Original dataset from https://webrobots.io/kickstarter-datasets/  

Used spacy and NLTK to tokenize, lemmatize, and remove stopwords.  

  Tokeization -> Break up sentences into individual words
  
  Lemmatization -> Reducing words to their base words, e.g. mice -> mouse, printed -> print  
  
  Stopword removal -> Removing words and punctuation that aren't meaningful on their own (eg, words like 'and, to, the')   
